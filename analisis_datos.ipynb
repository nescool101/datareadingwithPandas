{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Análisis de Datos - Machine Learning\n",
        "\n",
        "Este notebook contiene el análisis completo de datos con modelos de machine learning.\n",
        "\n",
        "## Pasos del análisis:\n",
        "1. Carga de datos\n",
        "2. Conversión a JSON y conteo por cuenta\n",
        "3. Análisis Exploratorio de Datos (EDA)\n",
        "4. Filtrado de datos (Tarifa NBO y Rentabilizacion)\n",
        "5. Preparación de datos\n",
        "6. Entrenamiento de modelos (Regresión Logística, Random Forest, XGBoost)\n",
        "7. Validación cruzada\n",
        "8. Grid Search para árboles de decisión\n",
        "9. Comparación y selección del mejor modelo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 1: Importación de librerías\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
        "import warnings\n",
        "import json\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ Librerías importadas correctamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 2: Carga de datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "archivo = \"Total_Mes_Act_Datos completos.csv\"\n",
        "output_dir = 'resultados'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(\"Cargando datos...\")\n",
        "try:\n",
        "    df = pd.read_csv(\n",
        "        archivo, \n",
        "        low_memory=False,\n",
        "        encoding='utf-8',\n",
        "        on_bad_lines='skip',\n",
        "        sep=',',\n",
        "        quotechar='\"'\n",
        "    )\n",
        "    print(f\"✓ Datos cargados: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Intentando con encoding alternativo...\")\n",
        "    df = pd.read_csv(\n",
        "        archivo,\n",
        "        low_memory=False,\n",
        "        encoding='latin-1',\n",
        "        on_bad_lines='skip',\n",
        "        sep=';',\n",
        "        quotechar='\"'\n",
        "    )\n",
        "    print(f\"✓ Datos cargados: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 3: Conversión a JSON y Conteo por Cuenta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Convirtiendo datos a JSON (muestra de 100,000 registros)...\")\n",
        "max_rows_json = min(100000, len(df))\n",
        "df_muestra = df.head(max_rows_json)\n",
        "\n",
        "ruta_json = os.path.join(output_dir, 'datos_completos.json')\n",
        "df_muestra.to_json(\n",
        "    ruta_json,\n",
        "    orient='records',\n",
        "    date_format='iso',\n",
        "    indent=2,\n",
        "    force_ascii=False\n",
        ")\n",
        "print(f\"✓ Archivo JSON guardado en '{ruta_json}'\")\n",
        "print(f\"Total de registros convertidos: {len(df_muestra)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "columna_cuenta = None\n",
        "\n",
        "for col in df.columns:\n",
        "    if 'cuenta' in col.lower():\n",
        "        columna_cuenta = col\n",
        "        break\n",
        "\n",
        "if columna_cuenta is None:\n",
        "    posibles = [col for col in df.columns if 'cuent' in col.lower() or 'account' in col.lower()]\n",
        "    if posibles:\n",
        "        columna_cuenta = posibles[0]\n",
        "\n",
        "if columna_cuenta:\n",
        "    print(f\"Columna seleccionada: {columna_cuenta}\")\n",
        "    conteo = df[columna_cuenta].value_counts().reset_index()\n",
        "    conteo.columns = ['Cuenta', 'Frecuencia']\n",
        "    conteo = conteo.sort_values('Frecuencia', ascending=False)\n",
        "    \n",
        "    print(f\"\\nTotal de cuentas únicas: {len(conteo)}\")\n",
        "    print(f\"\\nTop 10 cuentas más frecuentes:\")\n",
        "    display(conteo.head(10))\n",
        "    \n",
        "    archivo_csv = os.path.join(output_dir, 'conteo_por_cuenta.csv')\n",
        "    conteo.to_csv(archivo_csv, index=False, encoding='utf-8')\n",
        "    print(f\"\\n✓ Conteo guardado en '{archivo_csv}'\")\n",
        "else:\n",
        "    print(\"No se encontró columna 'cuenta'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 4: Análisis Exploratorio de Datos (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Información del dataset:\")\n",
        "print(f\"Dimensiones: {df.shape}\")\n",
        "print(f\"\\nColumnas: {len(df.columns)}\")\n",
        "print(f\"\\nTipos de datos:\")\n",
        "print(df.dtypes.value_counts())\n",
        "\n",
        "print(f\"\\nValores faltantes:\")\n",
        "missing = df.isnull().sum()\n",
        "missing_pct = (missing / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Valores_Faltantes': missing,\n",
        "    'Porcentaje': missing_pct\n",
        "})\n",
        "missing_df = missing_df[missing_df['Valores_Faltantes'] > 0].sort_values('Porcentaje', ascending=False)\n",
        "\n",
        "if len(missing_df) > 0:\n",
        "    display(missing_df.head(20))\n",
        "else:\n",
        "    print(\"No hay valores faltantes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 5: Identificación y Filtrado de Datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "columna_tarifa = None\n",
        "columna_target = None\n",
        "\n",
        "for col in df.columns:\n",
        "    if 'tarifa' in col.lower() and 'nbo' in col.lower():\n",
        "        columna_tarifa = col\n",
        "        break\n",
        "\n",
        "if columna_tarifa is None:\n",
        "    posibles = [col for col in df.columns if 'tarifa' in col.lower() or 'nbo' in col.lower()]\n",
        "    if posibles:\n",
        "        columna_tarifa = posibles[0]\n",
        "\n",
        "for col in df.columns:\n",
        "    if 'rentabilizacion' in col.lower() or 'rentabiliz' in col.lower():\n",
        "        columna_target = col\n",
        "        break\n",
        "\n",
        "if columna_target is None:\n",
        "    posibles = [col for col in df.columns if 'rent' in col.lower()]\n",
        "    if posibles:\n",
        "        columna_target = posibles[0]\n",
        "\n",
        "print(f\"Columna Tarifa NBO: {columna_tarifa}\")\n",
        "print(f\"Columna Rentabilizacion: {columna_target}\")\n",
        "\n",
        "if columna_tarifa and columna_target:\n",
        "    df_filtrado = df[df[columna_tarifa].notna()].copy()\n",
        "    print(f\"\\nFilas después de filtrar por {columna_tarifa}: {len(df_filtrado)}\")\n",
        "    \n",
        "    df_filtrado = df_filtrado[df_filtrado[columna_target].notna()].copy()\n",
        "    print(f\"Filas después de filtrar por {columna_target}: {len(df_filtrado)}\")\n",
        "    \n",
        "    print(f\"\\nDistribución de la variable objetivo ({columna_target}):\")\n",
        "    display(df_filtrado[columna_target].value_counts())\n",
        "else:\n",
        "    print(\"\\nError: No se encontraron las columnas necesarias\")\n",
        "    df_filtrado = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 6: Preparación de Datos para Modelado\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if df_filtrado is not None and columna_target:\n",
        "    y = df_filtrado[columna_target].copy()\n",
        "    X = df_filtrado.drop(columns=[columna_target])\n",
        "    \n",
        "    columnas_numericas = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    columnas_categoricas = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    \n",
        "    print(f\"Columnas numéricas: {len(columnas_numericas)}\")\n",
        "    print(f\"Columnas categóricas: {len(columnas_categoricas)}\")\n",
        "    \n",
        "    X_processed = X[columnas_numericas].copy()\n",
        "    \n",
        "    for col in columnas_categoricas:\n",
        "        if X[col].nunique() < 50:\n",
        "            le = LabelEncoder()\n",
        "            X_processed[col] = le.fit_transform(X[col].astype(str).fillna('Missing'))\n",
        "    \n",
        "    X_processed = X_processed.fillna(X_processed.median())\n",
        "    \n",
        "    if isinstance(y.dtype, object) or y.dtype == 'object':\n",
        "        le_target = LabelEncoder()\n",
        "        y = le_target.fit_transform(y.astype(str))\n",
        "    else:\n",
        "        y = y.astype(int)\n",
        "    \n",
        "    print(f\"\\nShape final: X={X_processed.shape}, y={y.shape}\")\n",
        "    print(f\"Clases en y: {np.unique(y)}\")\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_processed, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n✓ Datos preparados:\")\n",
        "    print(f\"  Train: {X_train.shape[0]} muestras\")\n",
        "    print(f\"  Test: {X_test.shape[0]} muestras\")\n",
        "else:\n",
        "    print(\"Error: No se pueden preparar los datos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 7: Entrenamiento de Modelos\n",
        "\n",
        "Dividimos el flujo en subpasos utilizando lecturas por bloques de 50.000 filas para monitorear el progreso y reutilizar los artefactos generados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 8: Grid Search con muestreo estratificado\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'X_processed' not in globals() or 'y' not in globals():\n",
        "    raise RuntimeError('Es necesario ejecutar el Paso 7.1.1 antes de continuar.')\n",
        "\n",
        "n_muestra = min(300000, len(X_processed))\n",
        "print(f\"Tomando muestra estratificada de {n_muestra} registros para grid search...\", flush=True)\n",
        "\n",
        "X_muestra, _, y_muestra, _ = train_test_split(\n",
        "    X_processed,\n",
        "    y,\n",
        "    train_size=n_muestra,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "X_train_grid, X_test_grid, y_train_grid, y_test_grid = train_test_split(\n",
        "    X_muestra,\n",
        "    y_muestra,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_muestra\n",
        ")\n",
        "\n",
        "print(f\"  Train: {X_train_grid.shape[0]} muestras\", flush=True)\n",
        "print(f\"  Test: {X_test_grid.shape[0]} muestras\", flush=True)\n",
        "\n",
        "cv_grid = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "print('\\nGrid Search - Random Forest', flush=True)\n",
        "parametros_rf = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "rf_base = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "modelo_grid_rf = GridSearchCV(\n",
        "    rf_base,\n",
        "    parametros_rf,\n",
        "    cv=cv_grid,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "modelo_grid_rf.fit(X_train_grid, y_train_grid)\n",
        "\n",
        "resultado_grid_rf = {\n",
        "    'best_params': modelo_grid_rf.best_params_,\n",
        "    'best_score': float(modelo_grid_rf.best_score_)\n",
        "}\n",
        "\n",
        "with open(os.path.join(output_dir, 'grid_rf.json'), 'w', encoding='utf-8') as archivo_rf:\n",
        "    json.dump(resultado_grid_rf, archivo_rf, indent=2)\n",
        "\n",
        "print(f\"✓ Mejor combinación RF: {resultado_grid_rf['best_params']} (score {resultado_grid_rf['best_score']:.4f})\", flush=True)\n",
        "\n",
        "print('\\nGrid Search - XGBoost', flush=True)\n",
        "parametros_xgb = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5],\n",
        "    'learning_rate': [0.1, 0.2],\n",
        "    'subsample': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "xgb_base = XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss')\n",
        "modelo_grid_xgb = GridSearchCV(\n",
        "    xgb_base,\n",
        "    parametros_xgb,\n",
        "    cv=cv_grid,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "modelo_grid_xgb.fit(X_train_grid, y_train_grid)\n",
        "\n",
        "resultado_grid_xgb = {\n",
        "    'best_params': modelo_grid_xgb.best_params_,\n",
        "    'best_score': float(modelo_grid_xgb.best_score_)\n",
        "}\n",
        "\n",
        "with open(os.path.join(output_dir, 'grid_xgb.json'), 'w', encoding='utf-8') as archivo_xgb:\n",
        "    json.dump(resultado_grid_xgb, archivo_xgb, indent=2)\n",
        "\n",
        "print(f\"✓ Mejor combinación XGBoost: {resultado_grid_xgb['best_params']} (score {resultado_grid_xgb['best_score']:.4f})\", flush=True)\n",
        "print('\\n✓ Paso 8 completado', flush=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 9: Resumen de resultados de Grid Search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ruta_grid_rf = os.path.join(output_dir, 'grid_rf.json')\n",
        "ruta_grid_xgb = os.path.join(output_dir, 'grid_xgb.json')\n",
        "\n",
        "with open(ruta_grid_rf, 'r', encoding='utf-8') as archivo_rf:\n",
        "    resumen_rf = json.load(archivo_rf)\n",
        "\n",
        "with open(ruta_grid_xgb, 'r', encoding='utf-8') as archivo_xgb:\n",
        "    resumen_xgb = json.load(archivo_xgb)\n",
        "\n",
        "print('Resultados guardados de Grid Search:', flush=True)\n",
        "print(f\"  Random Forest -> score CV: {resumen_rf['best_score']:.4f} | parámetros: {resumen_rf['best_params']}\", flush=True)\n",
        "print(f\"  XGBoost       -> score CV: {resumen_xgb['best_score']:.4f} | parámetros: {resumen_xgb['best_params']}\", flush=True)\n",
        "\n",
        "if 'modelo_grid_rf' in globals() and 'modelo_grid_xgb' in globals():\n",
        "    print('\\nEvaluación rápida en el conjunto de prueba retenido de la muestra:', flush=True)\n",
        "    pred_rf = modelo_grid_rf.best_estimator_.predict(X_test_grid)\n",
        "    pred_xgb = modelo_grid_xgb.best_estimator_.predict(X_test_grid)\n",
        "    print(f\"  Random Forest -> accuracy test: {accuracy_score(y_test_grid, pred_rf):.4f}\", flush=True)\n",
        "    print(f\"  XGBoost       -> accuracy test: {accuracy_score(y_test_grid, pred_xgb):.4f}\", flush=True)\n",
        "else:\n",
        "    print('\\nPara evaluar en test es necesario volver a ejecutar el Paso 8 en esta sesión.', flush=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 10: Comparación y Conclusión\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ruta_grid_rf = os.path.join(output_dir, 'grid_rf.json')\n",
        "ruta_grid_xgb = os.path.join(output_dir, 'grid_xgb.json')\n",
        "ruta_cv_lr = os.path.join(output_dir, 'resultado_cv_lr.json')\n",
        "\n",
        "faltantes = [ruta for ruta in [ruta_grid_rf, ruta_grid_xgb, ruta_cv_lr] if not os.path.exists(ruta)]\n",
        "if faltantes:\n",
        "    raise FileNotFoundError(f\"No se encontraron los archivos requeridos: {faltantes}\")\n",
        "\n",
        "with open(ruta_grid_rf, 'r', encoding='utf-8') as archivo_rf:\n",
        "    resumen_rf = json.load(archivo_rf)\n",
        "\n",
        "with open(ruta_grid_xgb, 'r', encoding='utf-8') as archivo_xgb:\n",
        "    resumen_xgb = json.load(archivo_xgb)\n",
        "\n",
        "with open(ruta_cv_lr, 'r', encoding='utf-8') as archivo_lr:\n",
        "    resumen_lr = json.load(archivo_lr)\n",
        "\n",
        "print('COMPARACIÓN DE MODELOS', flush=True)\n",
        "print('=' * 80, flush=True)\n",
        "print('\\nValidación cruzada - Regresión logística:', flush=True)\n",
        "print(f\"  Accuracy promedio: {resumen_lr['mean']:.4f} (+/- {resumen_lr['std'] * 2:.4f})\", flush=True)\n",
        "\n",
        "print('\\nGrid Search - Modelos de árboles:', flush=True)\n",
        "print(f\"  Random Forest -> score CV: {resumen_rf['best_score']:.4f} | parámetros: {resumen_rf['best_params']}\", flush=True)\n",
        "print(f\"  XGBoost       -> score CV: {resumen_xgb['best_score']:.4f} | parámetros: {resumen_xgb['best_params']}\", flush=True)\n",
        "\n",
        "mejor_arbol = 'RandomForest' if resumen_rf['best_score'] > resumen_xgb['best_score'] else 'XGBoost'\n",
        "\n",
        "print('\\n' + '=' * 80, flush=True)\n",
        "print('CONCLUSIÓN', flush=True)\n",
        "print('=' * 80, flush=True)\n",
        "print(f\"  Mejor modelo entre árboles (según Grid Search): {mejor_arbol}\", flush=True)\n",
        "\n",
        "conclusion = {\n",
        "    'cv_lr_mean': resumen_lr['mean'],\n",
        "    'cv_lr_std': resumen_lr['std'],\n",
        "    'grid_rf': resumen_rf,\n",
        "    'grid_xgb': resumen_xgb,\n",
        "    'best_tree_model': mejor_arbol\n",
        "}\n",
        "\n",
        "ruta_conclusion = os.path.join(output_dir, 'conclusion_paso_10.json')\n",
        "with open(ruta_conclusion, 'w', encoding='utf-8') as archivo_conclusion:\n",
        "    json.dump(conclusion, archivo_conclusion, indent=2)\n",
        "\n",
        "print(f\"\\n✓ Conclusión almacenada en: {ruta_conclusion}\", flush=True)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
