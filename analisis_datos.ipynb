{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Análisis de Datos - Machine Learning\n",
        "\n",
        "Este notebook contiene el análisis completo de datos con modelos de machine learning.\n",
        "\n",
        "## Pasos del análisis:\n",
        "1. Carga de datos\n",
        "2. Conversión a JSON y conteo por cuenta\n",
        "3. Análisis Exploratorio de Datos (EDA)\n",
        "4. Filtrado de datos (Tarifa NBO y Rentabilizacion)\n",
        "5. Preparación de datos\n",
        "6. Entrenamiento de modelos (Regresión Logística, Random Forest, XGBoost)\n",
        "7. Validación cruzada\n",
        "8. Grid Search para árboles de decisión\n",
        "9. Comparación y selección del mejor modelo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 1: Importación de librerías\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
        "import warnings\n",
        "import json\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ Librerías importadas correctamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 2: Carga de datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "archivo = \"Total_Mes_Act_Datos completos.csv\"\n",
        "output_dir = 'resultados'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(\"Cargando datos...\")\n",
        "try:\n",
        "    df = pd.read_csv(\n",
        "        archivo, \n",
        "        low_memory=False,\n",
        "        encoding='utf-8',\n",
        "        on_bad_lines='skip',\n",
        "        sep=',',\n",
        "        quotechar='\"'\n",
        "    )\n",
        "    print(f\"✓ Datos cargados: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Intentando con encoding alternativo...\")\n",
        "    df = pd.read_csv(\n",
        "        archivo,\n",
        "        low_memory=False,\n",
        "        encoding='latin-1',\n",
        "        on_bad_lines='skip',\n",
        "        sep=';',\n",
        "        quotechar='\"'\n",
        "    )\n",
        "    print(f\"✓ Datos cargados: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 3: Conversión a JSON y Conteo por Cuenta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Convirtiendo datos a JSON (muestra de 100,000 registros)...\")\n",
        "max_rows_json = min(100000, len(df))\n",
        "df_muestra = df.head(max_rows_json)\n",
        "\n",
        "ruta_json = os.path.join(output_dir, 'datos_completos.json')\n",
        "df_muestra.to_json(\n",
        "    ruta_json,\n",
        "    orient='records',\n",
        "    date_format='iso',\n",
        "    indent=2,\n",
        "    force_ascii=False\n",
        ")\n",
        "print(f\"✓ Archivo JSON guardado en '{ruta_json}'\")\n",
        "print(f\"Total de registros convertidos: {len(df_muestra)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "columna_cuenta = None\n",
        "\n",
        "for col in df.columns:\n",
        "    if 'cuenta' in col.lower():\n",
        "        columna_cuenta = col\n",
        "        break\n",
        "\n",
        "if columna_cuenta is None:\n",
        "    posibles = [col for col in df.columns if 'cuent' in col.lower() or 'account' in col.lower()]\n",
        "    if posibles:\n",
        "        columna_cuenta = posibles[0]\n",
        "\n",
        "if columna_cuenta:\n",
        "    print(f\"Columna seleccionada: {columna_cuenta}\")\n",
        "    conteo = df[columna_cuenta].value_counts().reset_index()\n",
        "    conteo.columns = ['Cuenta', 'Frecuencia']\n",
        "    conteo = conteo.sort_values('Frecuencia', ascending=False)\n",
        "    \n",
        "    print(f\"\\nTotal de cuentas únicas: {len(conteo)}\")\n",
        "    print(f\"\\nTop 10 cuentas más frecuentes:\")\n",
        "    display(conteo.head(10))\n",
        "    \n",
        "    archivo_csv = os.path.join(output_dir, 'conteo_por_cuenta.csv')\n",
        "    conteo.to_csv(archivo_csv, index=False, encoding='utf-8')\n",
        "    print(f\"\\n✓ Conteo guardado en '{archivo_csv}'\")\n",
        "else:\n",
        "    print(\"No se encontró columna 'cuenta'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 4: Análisis Exploratorio de Datos (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Información del dataset:\")\n",
        "print(f\"Dimensiones: {df.shape}\")\n",
        "print(f\"\\nColumnas: {len(df.columns)}\")\n",
        "print(f\"\\nTipos de datos:\")\n",
        "print(df.dtypes.value_counts())\n",
        "\n",
        "print(f\"\\nValores faltantes:\")\n",
        "missing = df.isnull().sum()\n",
        "missing_pct = (missing / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Valores_Faltantes': missing,\n",
        "    'Porcentaje': missing_pct\n",
        "})\n",
        "missing_df = missing_df[missing_df['Valores_Faltantes'] > 0].sort_values('Porcentaje', ascending=False)\n",
        "\n",
        "if len(missing_df) > 0:\n",
        "    display(missing_df.head(20))\n",
        "else:\n",
        "    print(\"No hay valores faltantes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 5: Identificación y Filtrado de Datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "columna_tarifa = None\n",
        "columna_target = None\n",
        "\n",
        "for col in df.columns:\n",
        "    if 'tarifa' in col.lower() and 'nbo' in col.lower():\n",
        "        columna_tarifa = col\n",
        "        break\n",
        "\n",
        "if columna_tarifa is None:\n",
        "    posibles = [col for col in df.columns if 'tarifa' in col.lower() or 'nbo' in col.lower()]\n",
        "    if posibles:\n",
        "        columna_tarifa = posibles[0]\n",
        "\n",
        "for col in df.columns:\n",
        "    if 'rentabilizacion' in col.lower() or 'rentabiliz' in col.lower():\n",
        "        columna_target = col\n",
        "        break\n",
        "\n",
        "if columna_target is None:\n",
        "    posibles = [col for col in df.columns if 'rent' in col.lower()]\n",
        "    if posibles:\n",
        "        columna_target = posibles[0]\n",
        "\n",
        "print(f\"Columna Tarifa NBO: {columna_tarifa}\")\n",
        "print(f\"Columna Rentabilizacion: {columna_target}\")\n",
        "\n",
        "if columna_tarifa and columna_target:\n",
        "    df_filtrado = df[df[columna_tarifa].notna()].copy()\n",
        "    print(f\"\\nFilas después de filtrar por {columna_tarifa}: {len(df_filtrado)}\")\n",
        "    \n",
        "    df_filtrado = df_filtrado[df_filtrado[columna_target].notna()].copy()\n",
        "    print(f\"Filas después de filtrar por {columna_target}: {len(df_filtrado)}\")\n",
        "    \n",
        "    print(f\"\\nDistribución de la variable objetivo ({columna_target}):\")\n",
        "    display(df_filtrado[columna_target].value_counts())\n",
        "else:\n",
        "    print(\"\\nError: No se encontraron las columnas necesarias\")\n",
        "    df_filtrado = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 6: Preparación de Datos para Modelado\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if df_filtrado is not None and columna_target:\n",
        "    y = df_filtrado[columna_target].copy()\n",
        "    X = df_filtrado.drop(columns=[columna_target])\n",
        "    \n",
        "    columnas_numericas = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    columnas_categoricas = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    \n",
        "    print(f\"Columnas numéricas: {len(columnas_numericas)}\")\n",
        "    print(f\"Columnas categóricas: {len(columnas_categoricas)}\")\n",
        "    \n",
        "    X_processed = X[columnas_numericas].copy()\n",
        "    \n",
        "    for col in columnas_categoricas:\n",
        "        if X[col].nunique() < 50:\n",
        "            le = LabelEncoder()\n",
        "            X_processed[col] = le.fit_transform(X[col].astype(str).fillna('Missing'))\n",
        "    \n",
        "    X_processed = X_processed.fillna(X_processed.median())\n",
        "    \n",
        "    if isinstance(y.dtype, object) or y.dtype == 'object':\n",
        "        le_target = LabelEncoder()\n",
        "        y = le_target.fit_transform(y.astype(str))\n",
        "    else:\n",
        "        y = y.astype(int)\n",
        "    \n",
        "    print(f\"\\nShape final: X={X_processed.shape}, y={y.shape}\")\n",
        "    print(f\"Clases en y: {np.unique(y)}\")\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_processed, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n✓ Datos preparados:\")\n",
        "    print(f\"  Train: {X_train.shape[0]} muestras\")\n",
        "    print(f\"  Test: {X_test.shape[0]} muestras\")\n",
        "else:\n",
        "    print(\"Error: No se pueden preparar los datos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 7: Entrenamiento de Modelos\n",
        "\n",
        "Dividimos el flujo en subpasos utilizando lecturas por bloques de 50.000 filas para monitorear el progreso y reutilizar los artefactos generados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 8: Grid Search con muestreo estratificado\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'X_processed' not in globals() or 'y' not in globals():\n",
        "    raise RuntimeError('Es necesario ejecutar el Paso 7.1.1 antes de continuar.')\n",
        "\n",
        "n_muestra = min(300000, len(X_processed))\n",
        "print(f\"Tomando muestra estratificada de {n_muestra} registros para grid search...\", flush=True)\n",
        "\n",
        "X_muestra, _, y_muestra, _ = train_test_split(\n",
        "    X_processed,\n",
        "    y,\n",
        "    train_size=n_muestra,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "X_train_grid, X_test_grid, y_train_grid, y_test_grid = train_test_split(\n",
        "    X_muestra,\n",
        "    y_muestra,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_muestra\n",
        ")\n",
        "\n",
        "print(f\"  Train: {X_train_grid.shape[0]} muestras\", flush=True)\n",
        "print(f\"  Test: {X_test_grid.shape[0]} muestras\", flush=True)\n",
        "\n",
        "cv_grid = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "print('\\nGrid Search - Random Forest', flush=True)\n",
        "parametros_rf = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "rf_base = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "modelo_grid_rf = GridSearchCV(\n",
        "    rf_base,\n",
        "    parametros_rf,\n",
        "    cv=cv_grid,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "modelo_grid_rf.fit(X_train_grid, y_train_grid)\n",
        "\n",
        "resultado_grid_rf = {\n",
        "    'best_params': modelo_grid_rf.best_params_,\n",
        "    'best_score': float(modelo_grid_rf.best_score_)\n",
        "}\n",
        "\n",
        "with open(os.path.join(output_dir, 'grid_rf.json'), 'w', encoding='utf-8') as archivo_rf:\n",
        "    json.dump(resultado_grid_rf, archivo_rf, indent=2)\n",
        "\n",
        "print(f\"✓ Mejor combinación RF: {resultado_grid_rf['best_params']} (score {resultado_grid_rf['best_score']:.4f})\", flush=True)\n",
        "\n",
        "print('\\nGrid Search - XGBoost', flush=True)\n",
        "parametros_xgb = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5],\n",
        "    'learning_rate': [0.1, 0.2],\n",
        "    'subsample': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "xgb_base = XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss')\n",
        "modelo_grid_xgb = GridSearchCV(\n",
        "    xgb_base,\n",
        "    parametros_xgb,\n",
        "    cv=cv_grid,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "modelo_grid_xgb.fit(X_train_grid, y_train_grid)\n",
        "\n",
        "resultado_grid_xgb = {\n",
        "    'best_params': modelo_grid_xgb.best_params_,\n",
        "    'best_score': float(modelo_grid_xgb.best_score_)\n",
        "}\n",
        "\n",
        "with open(os.path.join(output_dir, 'grid_xgb.json'), 'w', encoding='utf-8') as archivo_xgb:\n",
        "    json.dump(resultado_grid_xgb, archivo_xgb, indent=2)\n",
        "\n",
        "print(f\"✓ Mejor combinación XGBoost: {resultado_grid_xgb['best_params']} (score {resultado_grid_xgb['best_score']:.4f})\", flush=True)\n",
        "print('\\n✓ Paso 8 completado', flush=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 9: Resumen de resultados de Grid Search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ruta_grid_rf = os.path.join(output_dir, 'grid_rf.json')\n",
        "ruta_grid_xgb = os.path.join(output_dir, 'grid_xgb.json')\n",
        "\n",
        "with open(ruta_grid_rf, 'r', encoding='utf-8') as archivo_rf:\n",
        "    resumen_rf = json.load(archivo_rf)\n",
        "\n",
        "with open(ruta_grid_xgb, 'r', encoding='utf-8') as archivo_xgb:\n",
        "    resumen_xgb = json.load(archivo_xgb)\n",
        "\n",
        "print('Resultados guardados de Grid Search:', flush=True)\n",
        "print(f\"  Random Forest -> score CV: {resumen_rf['best_score']:.4f} | parámetros: {resumen_rf['best_params']}\", flush=True)\n",
        "print(f\"  XGBoost       -> score CV: {resumen_xgb['best_score']:.4f} | parámetros: {resumen_xgb['best_params']}\", flush=True)\n",
        "\n",
        "if 'modelo_grid_rf' in globals() and 'modelo_grid_xgb' in globals():\n",
        "    print('\\nEvaluación rápida en el conjunto de prueba retenido de la muestra:', flush=True)\n",
        "    pred_rf = modelo_grid_rf.best_estimator_.predict(X_test_grid)\n",
        "    pred_xgb = modelo_grid_xgb.best_estimator_.predict(X_test_grid)\n",
        "    print(f\"  Random Forest -> accuracy test: {accuracy_score(y_test_grid, pred_rf):.4f}\", flush=True)\n",
        "    print(f\"  XGBoost       -> accuracy test: {accuracy_score(y_test_grid, pred_xgb):.4f}\", flush=True)\n",
        "else:\n",
        "    print('\\nPara evaluar en test es necesario volver a ejecutar el Paso 8 en esta sesión.', flush=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 10: Comparación y Conclusión\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ruta_grid_rf = os.path.join(output_dir, 'grid_rf.json')\n",
        "ruta_grid_xgb = os.path.join(output_dir, 'grid_xgb.json')\n",
        "ruta_cv_lr = os.path.join(output_dir, 'resultado_cv_lr.json')\n",
        "\n",
        "faltantes = [ruta for ruta in [ruta_grid_rf, ruta_grid_xgb, ruta_cv_lr] if not os.path.exists(ruta)]\n",
        "if faltantes:\n",
        "    raise FileNotFoundError(f\"No se encontraron los archivos requeridos: {faltantes}\")\n",
        "\n",
        "with open(ruta_grid_rf, 'r', encoding='utf-8') as archivo_rf:\n",
        "    resumen_rf = json.load(archivo_rf)\n",
        "\n",
        "with open(ruta_grid_xgb, 'r', encoding='utf-8') as archivo_xgb:\n",
        "    resumen_xgb = json.load(archivo_xgb)\n",
        "\n",
        "with open(ruta_cv_lr, 'r', encoding='utf-8') as archivo_lr:\n",
        "    resumen_lr = json.load(archivo_lr)\n",
        "\n",
        "print('COMPARACIÓN DE MODELOS', flush=True)\n",
        "print('=' * 80, flush=True)\n",
        "print('\\nValidación cruzada - Regresión logística:', flush=True)\n",
        "print(f\"  Accuracy promedio: {resumen_lr['mean']:.4f} (+/- {resumen_lr['std'] * 2:.4f})\", flush=True)\n",
        "\n",
        "print('\\nGrid Search - Modelos de árboles:', flush=True)\n",
        "print(f\"  Random Forest -> score CV: {resumen_rf['best_score']:.4f} | parámetros: {resumen_rf['best_params']}\", flush=True)\n",
        "print(f\"  XGBoost       -> score CV: {resumen_xgb['best_score']:.4f} | parámetros: {resumen_xgb['best_params']}\", flush=True)\n",
        "\n",
        "mejor_arbol = 'RandomForest' if resumen_rf['best_score'] > resumen_xgb['best_score'] else 'XGBoost'\n",
        "\n",
        "print('\\n' + '=' * 80, flush=True)\n",
        "print('CONCLUSIÓN', flush=True)\n",
        "print('=' * 80, flush=True)\n",
        "print(f\"  Mejor modelo entre árboles (según Grid Search): {mejor_arbol}\", flush=True)\n",
        "\n",
        "conclusion = {\n",
        "    'cv_lr_mean': resumen_lr['mean'],\n",
        "    'cv_lr_std': resumen_lr['std'],\n",
        "    'grid_rf': resumen_rf,\n",
        "    'grid_xgb': resumen_xgb,\n",
        "    'best_tree_model': mejor_arbol\n",
        "}\n",
        "\n",
        "ruta_conclusion = os.path.join(output_dir, 'conclusion_paso_10.json')\n",
        "with open(ruta_conclusion, 'w', encoding='utf-8') as archivo_conclusion:\n",
        "    json.dump(conclusion, archivo_conclusion, indent=2)\n",
        "\n",
        "print(f\"\\n✓ Conclusión almacenada en: {ruta_conclusion}\", flush=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 11: Análisis de Importancia de Variables y Visualizaciones\n",
        "\n",
        "En este paso analizamos las variables más importantes usando el mejor modelo (XGBoost) y creamos visualizaciones del análisis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from xgboost import plot_tree, plot_importance\n",
        "import graphviz\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
        "from sklearn.tree import export_graphviz\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print('=' * 80)\n",
        "print('PASO 11: ANÁLISIS DE IMPORTANCIA Y VISUALIZACIONES')\n",
        "print('=' * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\nCargando datos preparados y resultados...', flush=True)\n",
        "\n",
        "ruta_datos = os.path.join(output_dir, 'datos_preparados_7_1.pkl')\n",
        "with open(ruta_datos, 'rb') as f:\n",
        "    datos = pickle.load(f)\n",
        "\n",
        "X_train_scaled = datos['X_train_scaled']\n",
        "X_test_scaled = datos['X_test_scaled']\n",
        "y_train = datos['y_train']\n",
        "y_test = datos['y_test']\n",
        "\n",
        "print(f'✓ Datos cargados: Train={X_train_scaled.shape[0]}, Test={X_test_scaled.shape[0]}', flush=True)\n",
        "\n",
        "with open(os.path.join(output_dir, 'grid_xgb.json'), 'r', encoding='utf-8') as f:\n",
        "    grid_xgb_params = json.load(f)\n",
        "\n",
        "print(f'✓ Parámetros óptimos XGBoost cargados: {grid_xgb_params[\"best_params\"]}', flush=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\nEntrenando modelo XGBoost con parámetros óptimos...', flush=True)\n",
        "\n",
        "xgb_optimizado = XGBClassifier(\n",
        "    n_estimators=grid_xgb_params['best_params']['n_estimators'],\n",
        "    max_depth=grid_xgb_params['best_params']['max_depth'],\n",
        "    learning_rate=grid_xgb_params['best_params']['learning_rate'],\n",
        "    subsample=grid_xgb_params['best_params']['subsample'],\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "xgb_optimizado.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_xgb = xgb_optimizado.predict(X_test_scaled)\n",
        "y_pred_proba_xgb = xgb_optimizado.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "print(f'✓ Modelo entrenado - Accuracy en test: {accuracy_xgb:.4f}', flush=True)\n",
        "\n",
        "ruta_modelo_xgb = os.path.join(output_dir, 'modelo_xgb_optimizado.pkl')\n",
        "with open(ruta_modelo_xgb, 'wb') as f:\n",
        "    pickle.dump(xgb_optimizado, f)\n",
        "print(f'✓ Modelo guardado en: {ruta_modelo_xgb}', flush=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.1: Importancia de Características\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\nAnalizando importancia de características...', flush=True)\n",
        "\n",
        "importancia = xgb_optimizado.feature_importances_\n",
        "feature_names = [f'Feature_{i}' for i in range(len(importancia))]\n",
        "\n",
        "df_importancia = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importancia\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(f'\\nTop 20 características más importantes:', flush=True)\n",
        "print(df_importancia.head(20).to_string(index=False), flush=True)\n",
        "\n",
        "df_importancia.to_csv(os.path.join(output_dir, 'importancia_caracteristicas.csv'), index=False)\n",
        "print(f'\\n✓ Importancia guardada en CSV', flush=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
        "\n",
        "top_n = 20\n",
        "df_top = df_importancia.head(top_n)\n",
        "\n",
        "axes[0, 0].barh(range(len(df_top)), df_top['Importance'].values, color='steelblue')\n",
        "axes[0, 0].set_yticks(range(len(df_top)))\n",
        "axes[0, 0].set_yticklabels(df_top['Feature'].values)\n",
        "axes[0, 0].set_xlabel('Importancia', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_title(f'Top {top_n} Características Más Importantes (XGBoost)', \n",
        "                     fontsize=14, fontweight='bold', pad=20)\n",
        "axes[0, 0].invert_yaxis()\n",
        "axes[0, 0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "axes[0, 1].bar(range(len(df_top)), df_top['Importance'].values, color='coral')\n",
        "axes[0, 1].set_xticks(range(len(df_top)))\n",
        "axes[0, 1].set_xticklabels(df_top['Feature'].values, rotation=45, ha='right')\n",
        "axes[0, 1].set_ylabel('Importancia', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_title(f'Top {top_n} Características - Vista de Barras', \n",
        "                     fontsize=14, fontweight='bold', pad=20)\n",
        "axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "importancia_acumulada = df_importancia['Importance'].cumsum()\n",
        "axes[1, 0].plot(range(1, len(importancia_acumulada) + 1), importancia_acumulada.values, \n",
        "                marker='o', linewidth=2, markersize=4, color='green')\n",
        "axes[1, 0].axhline(y=0.8, color='r', linestyle='--', label='80% de importancia')\n",
        "axes[1, 0].set_xlabel('Número de Características', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Importancia Acumulada', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_title('Importancia Acumulada de Características', \n",
        "                     fontsize=14, fontweight='bold', pad=20)\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "n_features_80 = len(importancia_acumulada[importancia_acumulada <= 0.8])\n",
        "axes[1, 0].axvline(x=n_features_80, color='orange', linestyle='--', \n",
        "                   label=f'{n_features_80} features = 80%')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "axes[1, 1].pie(df_top['Importance'].values, labels=df_top['Feature'].values, \n",
        "               autopct='%1.1f%%', startangle=90)\n",
        "axes[1, 1].set_title(f'Distribución de Importancia - Top {top_n}', \n",
        "                     fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "ruta_grafica = os.path.join(output_dir, 'importancia_caracteristicas.png')\n",
        "plt.savefig(ruta_grafica, dpi=300, bbox_inches='tight')\n",
        "print(f'✓ Gráfica de importancia guardada en: {ruta_grafica}', flush=True)\n",
        "plt.show()\n",
        "\n",
        "print(f'\\nNúmero de características que explican el 80% de la importancia: {n_features_80}', flush=True)\n",
        "print(f'Características clave (top {n_features_80}):', flush=True)\n",
        "for i, row in df_importancia.head(n_features_80).iterrows():\n",
        "    print(f\"  {row['Feature']}: {row['Importance']:.6f}\", flush=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.2: Visualización de Árboles de Decisión\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\nVisualizando árboles de decisión individuales...', flush=True)\n",
        "\n",
        "try:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(30, 24))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, ax in enumerate(axes[:4]):\n",
        "        plot_tree(xgb_optimizado, num_trees=i, ax=ax, rankdir='LR')\n",
        "        ax.set_title(f'Árbol de Decisión #{i+1} (XGBoost)', fontsize=12, fontweight='bold', pad=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    ruta_arboles = os.path.join(output_dir, 'arboles_decision_xgb.png')\n",
        "    plt.savefig(ruta_arboles, dpi=300, bbox_inches='tight')\n",
        "    print(f'✓ Árboles de decisión guardados en: {ruta_arboles}', flush=True)\n",
        "    plt.show()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f'⚠ No se pudieron visualizar los árboles con plot_tree: {e}', flush=True)\n",
        "    print('Intentando método alternativo...', flush=True)\n",
        "    \n",
        "    try:\n",
        "        from xgboost import to_graphviz\n",
        "        \n",
        "        for i in range(min(3, xgb_optimizado.n_estimators)):\n",
        "            graph = to_graphviz(xgb_optimizado, num_trees=i)\n",
        "            ruta_arbol = os.path.join(output_dir, f'arbol_decision_{i+1}.dot')\n",
        "            graph.save(ruta_arbol)\n",
        "            print(f'✓ Árbol {i+1} guardado en formato DOT: {ruta_arbol}', flush=True)\n",
        "    except Exception as e2:\n",
        "        print(f'⚠ Método alternativo también falló: {e2}', flush=True)\n",
        "        print('Los árboles están disponibles en el modelo pero requieren herramientas adicionales para visualización.', flush=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.3: Matriz de Confusión y Métricas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_xgb)\n",
        "precision = precision_score(y_test, y_pred_xgb)\n",
        "recall = recall_score(y_test, y_pred_xgb)\n",
        "f1 = f1_score(y_test, y_pred_xgb)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba_xgb)\n",
        "\n",
        "print('\\n' + '=' * 80)\n",
        "print('MÉTRICAS DEL MODELO XGBOOST')\n",
        "print('=' * 80)\n",
        "print(f'Accuracy:  {accuracy_xgb:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall:    {recall:.4f}')\n",
        "print(f'F1-Score:  {f1:.4f}')\n",
        "print(f'ROC-AUC:   {roc_auc:.4f}')\n",
        "print('=' * 80)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n",
        "            xticklabels=['Clase 0', 'Clase 1'], \n",
        "            yticklabels=['Clase 0', 'Clase 1'])\n",
        "axes[0].set_xlabel('Predicción', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Real', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('Matriz de Confusión', fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "axes[1].bar(['Verdaderos\\nNegativos', 'Falsos\\nPositivos', 'Falsos\\nNegativos', 'Verdaderos\\nPositivos'],\n",
        "            [tn, fp, fn, tp], color=['green', 'orange', 'red', 'blue'], alpha=0.7)\n",
        "axes[1].set_ylabel('Cantidad', fontsize=12, fontweight='bold')\n",
        "axes[1].set_title('Desglose de la Matriz de Confusión', fontsize=14, fontweight='bold', pad=20)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "for i, v in enumerate([tn, fp, fn, tp]):\n",
        "    axes[1].text(i, v + max([tn, fp, fn, tp]) * 0.01, str(v), \n",
        "                ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
        "\n",
        "plt.tight_layout()\n",
        "ruta_confusion = os.path.join(output_dir, 'matriz_confusion_xgb.png')\n",
        "plt.savefig(ruta_confusion, dpi=300, bbox_inches='tight')\n",
        "print(f'\\n✓ Matriz de confusión guardada en: {ruta_confusion}', flush=True)\n",
        "plt.show()\n",
        "\n",
        "reporte = classification_report(y_test, y_pred_xgb, output_dict=True)\n",
        "df_reporte = pd.DataFrame(reporte).transpose()\n",
        "df_reporte.to_csv(os.path.join(output_dir, 'reporte_clasificacion_xgb.csv'))\n",
        "print('✓ Reporte de clasificación guardado en CSV', flush=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.4: Curvas ROC y Precision-Recall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_xgb)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba_xgb)\n",
        "pr_auc = auc(recall_curve, precision_curve)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "axes[0].plot(fpr, tpr, color='darkorange', lw=2, \n",
        "             label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
        "axes[0].set_xlabel('Tasa de Falsos Positivos', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Tasa de Verdaderos Positivos', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('Curva ROC', fontsize=14, fontweight='bold', pad=20)\n",
        "axes[0].legend(loc=\"lower right\", fontsize=11)\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "axes[1].plot(recall_curve, precision_curve, color='blue', lw=2,\n",
        "             label=f'Precision-Recall curve (AUC = {pr_auc:.4f})')\n",
        "baseline = len(y_test[y_test==1]) / len(y_test)\n",
        "axes[1].axhline(y=baseline, color='r', linestyle='--', \n",
        "                label=f'Baseline ({baseline:.4f})')\n",
        "axes[1].set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('Precision', fontsize=12, fontweight='bold')\n",
        "axes[1].set_title('Curva Precision-Recall', fontsize=14, fontweight='bold', pad=20)\n",
        "axes[1].legend(loc=\"lower left\", fontsize=11)\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "ruta_curvas = os.path.join(output_dir, 'curvas_roc_precision_recall.png')\n",
        "plt.savefig(ruta_curvas, dpi=300, bbox_inches='tight')\n",
        "print(f'✓ Curvas ROC y Precision-Recall guardadas en: {ruta_curvas}', flush=True)\n",
        "plt.show()\n",
        "\n",
        "print(f'\\nÁrea bajo la curva ROC: {roc_auc:.4f}', flush=True)\n",
        "print(f'Área bajo la curva Precision-Recall: {pr_auc:.4f}', flush=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.5: Resumen de Variables Clave\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\n' + '=' * 80)\n",
        "print('RESUMEN DE VARIABLES CLAVE PARA EL MODELO')\n",
        "print('=' * 80)\n",
        "\n",
        "n_variables_clave = min(15, len(df_importancia))\n",
        "variables_clave = df_importancia.head(n_variables_clave)\n",
        "\n",
        "print(f'\\nLas {n_variables_clave} variables más importantes son:\\n', flush=True)\n",
        "for idx, row in variables_clave.iterrows():\n",
        "    porcentaje = (row['Importance'] / df_importancia['Importance'].sum()) * 100\n",
        "    print(f\"  {idx+1:2d}. {row['Feature']:20s} - Importancia: {row['Importance']:.6f} ({porcentaje:.2f}%)\", flush=True)\n",
        "\n",
        "importancia_total_top = variables_clave['Importance'].sum()\n",
        "porcentaje_total = (importancia_total_top / df_importancia['Importance'].sum()) * 100\n",
        "print(f'\\nEstas {n_variables_clave} variables explican el {porcentaje_total:.2f}% de la importancia total.', flush=True)\n",
        "\n",
        "resumen_variables = {\n",
        "    'variables_clave': variables_clave['Feature'].tolist(),\n",
        "    'importancia_total': float(importancia_total_top),\n",
        "    'porcentaje_explicado': float(porcentaje_total),\n",
        "    'total_variables': len(df_importancia),\n",
        "    'variables_seleccionadas': n_variables_clave\n",
        "}\n",
        "\n",
        "ruta_resumen = os.path.join(output_dir, 'resumen_variables_clave.json')\n",
        "with open(ruta_resumen, 'w', encoding='utf-8') as f:\n",
        "    json.dump(resumen_variables, f, indent=2)\n",
        "\n",
        "print(f'\\n✓ Resumen de variables clave guardado en: {ruta_resumen}', flush=True)\n",
        "print('\\n✓ Paso 11 completado exitosamente', flush=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paso 12: Visualización de Árboles de Decisión (Estructura)\n",
        "\n",
        "Se generan imágenes de la estructura de los árboles de decisión:\n",
        "- Intento 1: Árboles internos del modelo XGBoost (si hay soporte de Graphviz).\n",
        "- Intento 2: Árbol surrogado (sklearn) entrenado sobre las variables más importantes, exportado como PNG.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Generando visualizaciones de árboles...', flush=True)\n",
        "\n",
        "# Asegurar df_importancia y nombres de features\n",
        "ruta_importancias = os.path.join(output_dir, 'importancia_caracteristicas.csv')\n",
        "if 'df_importancia' not in globals():\n",
        "    if os.path.exists(ruta_importancias):\n",
        "        df_importancia = pd.read_csv(ruta_importancias)\n",
        "    else:\n",
        "        raise RuntimeError('No se encuentra df_importancia. Ejecute el Paso 11 primero.')\n",
        "\n",
        "# Intento 1: Visualizar árboles de XGBoost directamente (requiere Graphviz)\n",
        "exitos_xgb = False\n",
        "try:\n",
        "    from xgboost import plot_tree as xgb_plot_tree\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(30, 24))\n",
        "    axes = axes.flatten()\n",
        "    for i, ax in enumerate(axes[:4]):\n",
        "        xgb_plot_tree(xgb_optimizado, num_trees=i, ax=ax, rankdir='LR')\n",
        "        ax.set_title(f'Estructura - Árbol XGBoost #{i+1}', fontsize=12, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    ruta_arboles_xgb = os.path.join(output_dir, 'estructura_arboles_xgb.png')\n",
        "    plt.savefig(ruta_arboles_xgb, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f'✓ Estructuras XGBoost guardadas en: {ruta_arboles_xgb}', flush=True)\n",
        "    exitos_xgb = True\n",
        "except Exception as e:\n",
        "    print(f'Nota: no fue posible visualizar árboles XGBoost directamente ({e}). Se usa árbol surrogado.', flush=True)\n",
        "\n",
        "# Intento 2: Árbol surrogado con sklearn sobre top-N variables más importantes\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree as sk_plot_tree\n",
        "\n",
        "# Tomar top-N variables\n",
        "top_n = 10\n",
        "indices_top = []\n",
        "for feat in df_importancia['Feature'].head(top_n).tolist():\n",
        "    # 'Feature_i' -> i\n",
        "    try:\n",
        "        idx = int(feat.split('_')[1])\n",
        "    except Exception:\n",
        "        idx = 0\n",
        "    indices_top.append(idx)\n",
        "\n",
        "# Preparar subconjunto de datos (usar escalados por consistencia)\n",
        "X_train_top = X_train_scaled[:, indices_top]\n",
        "X_test_top = X_test_scaled[:, indices_top]\n",
        "\n",
        "arbol_surrogado = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
        "arbol_surrogado.fit(X_train_top, y_train)\n",
        "\n",
        "fig = plt.figure(figsize=(24, 14))\n",
        "sk_plot_tree(\n",
        "    arbol_surrogado,\n",
        "    feature_names=[df_importancia['Feature'].iloc[i] for i in range(top_n)],\n",
        "    class_names=['Clase 0', 'Clase 1'],\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    fontsize=10\n",
        ")\n",
        "plt.title('Árbol de Decisión Surrogado (Top 10 variables)', fontsize=14, fontweight='bold')\n",
        "ruta_arbol_surrogado = os.path.join(output_dir, 'arbol_surrogado_top10.png')\n",
        "plt.savefig(ruta_arbol_surrogado, dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f'✓ Árbol surrogado guardado en: {ruta_arbol_surrogado}', flush=True)\n",
        "\n",
        "# Exportar también una versión más pequeña (profundidad 3) para lectura rápida\n",
        "arbol_surrogado_peq = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "arbol_surrogado_peq.fit(X_train_top, y_train)\n",
        "fig = plt.figure(figsize=(20, 12))\n",
        "sk_plot_tree(\n",
        "    arbol_surrogado_peq,\n",
        "    feature_names=[df_importancia['Feature'].iloc[i] for i in range(top_n)],\n",
        "    class_names=['Clase 0', 'Clase 1'],\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    fontsize=10\n",
        ")\n",
        "plt.title('Árbol Surrogado (Profundidad 3) - Top 10 variables', fontsize=14, fontweight='bold')\n",
        "ruta_arbol_surrogado_peq = os.path.join(output_dir, 'arbol_surrogado_top10_depth3.png')\n",
        "plt.savefig(ruta_arbol_surrogado_peq, dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f'✓ Árbol surrogado (profundidad 3) guardado en: {ruta_arbol_surrogado_peq}', flush=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 12.1: Visualizaciones EDA Completas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('=' * 80)\n",
        "print('VISUALIZACIONES EDA COMPLETAS')\n",
        "print('=' * 80)\n",
        "\n",
        "print('\\nCargando datos originales para EDA...', flush=True)\n",
        "\n",
        "archivo = 'Total_Mes_Act_Datos completos.csv'\n",
        "df_eda = pd.read_csv(\n",
        "    archivo, \n",
        "    low_memory=False,\n",
        "    encoding='utf-8',\n",
        "    on_bad_lines='skip',\n",
        "    sep=';',\n",
        "    quotechar='\"'\n",
        ")\n",
        "\n",
        "print(f'✓ Datos cargados: {df_eda.shape[0]} filas, {df_eda.shape[1]} columnas', flush=True)\n",
        "\n",
        "columna_tarifa = 'TARIFA_NBO'\n",
        "columna_rentabilizacion = 'Rentabilizo'\n",
        "\n",
        "df_eda['variable_objetivo'] = np.where(\n",
        "    (df_eda[columna_tarifa].notna()) & \n",
        "    (df_eda[columna_rentabilizacion].notna()),\n",
        "    1,\n",
        "    0\n",
        ")\n",
        "\n",
        "fig = plt.figure(figsize=(20, 16))\n",
        "\n",
        "distribucion = df_eda['variable_objetivo'].value_counts()\n",
        "ax1 = plt.subplot(3, 3, 1)\n",
        "distribucion.plot(kind='bar', color=['coral', 'steelblue'], ax=ax1)\n",
        "ax1.set_title('Distribución de Variable Objetivo', fontsize=12, fontweight='bold', pad=15)\n",
        "ax1.set_xlabel('Clase (0=No cumple, 1=Cumple)', fontsize=10, fontweight='bold')\n",
        "ax1.set_ylabel('Frecuencia', fontsize=10, fontweight='bold')\n",
        "ax1.set_xticklabels(['No cumple', 'Cumple'], rotation=0)\n",
        "for i, v in enumerate(distribucion.values):\n",
        "    ax1.text(i, v + max(distribucion.values) * 0.01, f'{v:,}', \n",
        "            ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "ax2 = plt.subplot(3, 3, 2)\n",
        "distribucion.plot(kind='pie', autopct='%1.1f%%', colors=['coral', 'steelblue'], ax=ax2, startangle=90)\n",
        "ax2.set_title('Distribución Porcentual', fontsize=12, fontweight='bold', pad=15)\n",
        "ax2.set_ylabel('')\n",
        "\n",
        "missing = df_eda.isnull().sum()\n",
        "missing_pct = (missing / len(df_eda)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Valores_Faltantes': missing,\n",
        "    'Porcentaje': missing_pct\n",
        "})\n",
        "missing_df = missing_df[missing_df['Valores_Faltantes'] > 0].sort_values('Porcentaje', ascending=False).head(15)\n",
        "\n",
        "ax3 = plt.subplot(3, 3, 3)\n",
        "missing_df['Porcentaje'].plot(kind='barh', color='orange', ax=ax3)\n",
        "ax3.set_title('Top 15 Columnas con Valores Faltantes', fontsize=12, fontweight='bold', pad=15)\n",
        "ax3.set_xlabel('Porcentaje de Valores Faltantes', fontsize=10, fontweight='bold')\n",
        "ax3.invert_yaxis()\n",
        "\n",
        "columnas_numericas = df_eda.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if len(columnas_numericas) > 0:\n",
        "    df_numericas = df_eda[columnas_numericas[:6]].copy()\n",
        "    \n",
        "    ax4 = plt.subplot(3, 3, 4)\n",
        "    df_numericas.boxplot(ax=ax4, rot=45)\n",
        "    ax4.set_title('Distribución de Variables Numéricas (Top 6)', fontsize=12, fontweight='bold', pad=15)\n",
        "    ax4.set_ylabel('Valor', fontsize=10, fontweight='bold')\n",
        "    \n",
        "    ax5 = plt.subplot(3, 3, 5)\n",
        "    df_numericas.hist(bins=30, ax=ax5, layout=(2, 3), figsize=(15, 10))\n",
        "    ax5.set_title('Histogramas de Variables Numéricas', fontsize=12, fontweight='bold', pad=15)\n",
        "\n",
        "if columna_tarifa in df_eda.columns:\n",
        "    ax6 = plt.subplot(3, 3, 6)\n",
        "    df_eda[columna_tarifa].notna().value_counts().plot(kind='bar', color=['red', 'green'], ax=ax6)\n",
        "    ax6.set_title(f'Distribución de {columna_tarifa}', fontsize=12, fontweight='bold', pad=15)\n",
        "    ax6.set_xlabel('Tiene Valor', fontsize=10, fontweight='bold')\n",
        "    ax6.set_ylabel('Frecuencia', fontsize=10, fontweight='bold')\n",
        "    ax6.set_xticklabels(['Faltante', 'Presente'], rotation=0)\n",
        "\n",
        "if columna_rentabilizacion in df_eda.columns:\n",
        "    ax7 = plt.subplot(3, 3, 7)\n",
        "    df_eda[columna_rentabilizacion].notna().value_counts().plot(kind='bar', color=['red', 'green'], ax=ax7)\n",
        "    ax7.set_title(f'Distribución de {columna_rentabilizacion}', fontsize=12, fontweight='bold', pad=15)\n",
        "    ax7.set_xlabel('Tiene Valor', fontsize=10, fontweight='bold')\n",
        "    ax7.set_ylabel('Frecuencia', fontsize=10, fontweight='bold')\n",
        "    ax7.set_xticklabels(['Faltante', 'Presente'], rotation=0)\n",
        "\n",
        "tipos_datos = df_eda.dtypes.value_counts()\n",
        "ax8 = plt.subplot(3, 3, 8)\n",
        "tipos_datos.plot(kind='bar', color='purple', ax=ax8)\n",
        "ax8.set_title('Distribución de Tipos de Datos', fontsize=12, fontweight='bold', pad=15)\n",
        "ax8.set_xlabel('Tipo de Dato', fontsize=10, fontweight='bold')\n",
        "ax8.set_ylabel('Cantidad de Columnas', fontsize=10, fontweight='bold')\n",
        "ax8.tick_params(axis='x', rotation=45)\n",
        "\n",
        "ax9 = plt.subplot(3, 3, 9)\n",
        "resumen_stats = {\n",
        "    'Total Filas': len(df_eda),\n",
        "    'Total Columnas': len(df_eda.columns),\n",
        "    'Columnas Numéricas': len(columnas_numericas),\n",
        "    'Columnas Categóricas': len(df_eda.columns) - len(columnas_numericas),\n",
        "    'Clase 1 (Cumple)': int(distribucion.get(1, 0)),\n",
        "    'Clase 0 (No cumple)': int(distribucion.get(0, 0))\n",
        "}\n",
        "stats_df = pd.DataFrame(list(resumen_stats.items()), columns=['Métrica', 'Valor'])\n",
        "stats_df.set_index('Métrica')['Valor'].plot(kind='barh', color='teal', ax=ax9)\n",
        "ax9.set_title('Resumen Estadístico del Dataset', fontsize=12, fontweight='bold', pad=15)\n",
        "ax9.set_xlabel('Valor', fontsize=10, fontweight='bold')\n",
        "ax9.invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "ruta_eda_completo = os.path.join(output_dir, 'eda_visualizaciones_completas.png')\n",
        "plt.savefig(ruta_eda_completo, dpi=300, bbox_inches='tight')\n",
        "print(f'\\n✓ Visualizaciones EDA completas guardadas en: {ruta_eda_completo}', flush=True)\n",
        "plt.show()\n",
        "\n",
        "print('\\n✓ Visualizaciones EDA completadas', flush=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 12.2: Árboles de Decisión Individuales con Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('=' * 80)\n",
        "print('GENERANDO ÁRBOLES DE DECISIÓN INDIVIDUALES')\n",
        "print('=' * 80)\n",
        "\n",
        "print('\\nCargando datos y modelo...', flush=True)\n",
        "\n",
        "if 'df_importancia' not in globals():\n",
        "    ruta_importancias = os.path.join(output_dir, 'importancia_caracteristicas.csv')\n",
        "    if os.path.exists(ruta_importancias):\n",
        "        df_importancia = pd.read_csv(ruta_importancias)\n",
        "    else:\n",
        "        raise RuntimeError('Ejecute el Paso 11 primero para generar importancias.')\n",
        "\n",
        "if 'xgb_optimizado' not in globals():\n",
        "    ruta_modelo = os.path.join(output_dir, 'modelo_xgb_optimizado.pkl')\n",
        "    if os.path.exists(ruta_modelo):\n",
        "        with open(ruta_modelo, 'rb') as f:\n",
        "            xgb_optimizado = pickle.load(f)\n",
        "    else:\n",
        "        raise RuntimeError('Ejecute el Paso 11 primero para generar el modelo.')\n",
        "\n",
        "if 'X_train_scaled' not in globals() or 'y_train' not in globals():\n",
        "    ruta_datos = os.path.join(output_dir, 'datos_preparados_7_1.pkl')\n",
        "    with open(ruta_datos, 'rb') as f:\n",
        "        datos = pickle.load(f)\n",
        "    X_train_scaled = datos['X_train_scaled']\n",
        "    y_train = datos['y_train']\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "\n",
        "top_n_vars = 10\n",
        "top_features = df_importancia.head(top_n_vars)['Feature'].tolist()\n",
        "\n",
        "indices_top = []\n",
        "for feat in top_features:\n",
        "    try:\n",
        "        idx = int(feat.split('_')[1])\n",
        "        indices_top.append(idx)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "if len(indices_top) == 0:\n",
        "    indices_top = list(range(min(top_n_vars, X_train_scaled.shape[1])))\n",
        "\n",
        "X_train_top = X_train_scaled[:, indices_top]\n",
        "feature_names_top = [df_importancia.iloc[i]['Feature'] for i in range(len(indices_top))]\n",
        "\n",
        "print(f'\\nGenerando árboles individuales con top {top_n_vars} variables...', flush=True)\n",
        "print(f'Variables utilizadas: {feature_names_top}', flush=True)\n",
        "\n",
        "profundidades = [3, 4, 5]\n",
        "for depth in profundidades:\n",
        "    arbol = DecisionTreeClassifier(max_depth=depth, random_state=42, min_samples_split=100)\n",
        "    arbol.fit(X_train_top, y_train)\n",
        "    \n",
        "    fig = plt.figure(figsize=(24, 14))\n",
        "    plot_tree(\n",
        "        arbol,\n",
        "        feature_names=feature_names_top,\n",
        "        class_names=['Clase 0\\n(No cumple)', 'Clase 1\\n(Cumple)'],\n",
        "        filled=True,\n",
        "        rounded=True,\n",
        "        fontsize=9,\n",
        "        proportion=True\n",
        "    )\n",
        "    plt.title(f'Árbol de Decisión - Profundidad {depth} - Top {top_n_vars} Variables\\nVariables: {\", \".join(feature_names_top[:5])}...', \n",
        "              fontsize=14, fontweight='bold', pad=20)\n",
        "    \n",
        "    ruta_arbol = os.path.join(output_dir, f'arbol_decision_depth{depth}_top{top_n_vars}.png')\n",
        "    plt.savefig(ruta_arbol, dpi=300, bbox_inches='tight')\n",
        "    print(f'✓ Árbol (profundidad {depth}) guardado en: {ruta_arbol}', flush=True)\n",
        "    plt.close()\n",
        "\n",
        "print('\\nGenerando árboles individuales por variable más importante...', flush=True)\n",
        "\n",
        "for i, (idx, row) in enumerate(df_importancia.head(5).iterrows()):\n",
        "    feat_name = row['Feature']\n",
        "    try:\n",
        "        feat_idx = int(feat_name.split('_')[1])\n",
        "    except:\n",
        "        feat_idx = i\n",
        "    \n",
        "    if feat_idx >= X_train_scaled.shape[1]:\n",
        "        continue\n",
        "    \n",
        "    indices_single = [feat_idx] + [j for j in indices_top[:4] if j != feat_idx]\n",
        "    X_train_single = X_train_scaled[:, indices_single[:5]]\n",
        "    feature_names_single = [df_importancia.iloc[j]['Feature'] if j < len(df_importancia) else f'Feature_{j}' \n",
        "                           for j in indices_single[:5]]\n",
        "    \n",
        "    arbol_single = DecisionTreeClassifier(max_depth=4, random_state=42, min_samples_split=100)\n",
        "    arbol_single.fit(X_train_single, y_train)\n",
        "    \n",
        "    fig = plt.figure(figsize=(22, 12))\n",
        "    plot_tree(\n",
        "        arbol_single,\n",
        "        feature_names=feature_names_single,\n",
        "        class_names=['Clase 0', 'Clase 1'],\n",
        "        filled=True,\n",
        "        rounded=True,\n",
        "        fontsize=9\n",
        "    )\n",
        "    plt.title(f'Árbol #{i+1} - Variable Principal: {feat_name}\\nImportancia: {row[\"Importance\"]:.4f} ({row[\"Importance\"]/df_importancia[\"Importance\"].sum()*100:.2f}%)', \n",
        "              fontsize=13, fontweight='bold', pad=20)\n",
        "    \n",
        "    ruta_arbol_single = os.path.join(output_dir, f'arbol_decision_var_{feat_name}.png')\n",
        "    plt.savefig(ruta_arbol_single, dpi=300, bbox_inches='tight')\n",
        "    print(f'✓ Árbol para {feat_name} guardado en: {ruta_arbol_single}', flush=True)\n",
        "    plt.close()\n",
        "\n",
        "print('\\n✓ Todos los árboles de decisión individuales generados exitosamente', flush=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('=' * 80)\n",
        "print('PASO 13: VISUALIZACIONES EDA INDIVIDUALES')\n",
        "print('=' * 80)\n",
        "\n",
        "print('\\nCargando datos originales para EDA...', flush=True)\n",
        "\n",
        "archivo = 'Total_Mes_Act_Datos completos.csv'\n",
        "df_eda = pd.read_csv(\n",
        "    archivo, \n",
        "    low_memory=False,\n",
        "    encoding='utf-8',\n",
        "    on_bad_lines='skip',\n",
        "    sep=';',\n",
        "    quotechar='\"'\n",
        ")\n",
        "\n",
        "print(f'✓ Datos cargados: {df_eda.shape[0]} filas, {df_eda.shape[1]} columnas', flush=True)\n",
        "\n",
        "columna_tarifa = 'TARIFA_NBO'\n",
        "columna_rentabilizacion = 'Rentabilizo'\n",
        "\n",
        "df_eda['variable_objetivo'] = np.where(\n",
        "    (df_eda[columna_tarifa].notna()) & \n",
        "    (df_eda[columna_rentabilizacion].notna()),\n",
        "    1,\n",
        "    0\n",
        ")\n",
        "\n",
        "distribucion = df_eda['variable_objetivo'].value_counts()\n",
        "\n",
        "print('\\nGenerando visualizaciones individuales...', flush=True)\n",
        "\n",
        "# 1. Distribución de Variable Objetivo (Barras)\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "distribucion.plot(kind='bar', color=['coral', 'steelblue'], ax=ax)\n",
        "ax.set_title('Distribución de Variable Objetivo', fontsize=14, fontweight='bold', pad=20)\n",
        "ax.set_xlabel('Clase (0=No cumple, 1=Cumple)', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Frecuencia', fontsize=12, fontweight='bold')\n",
        "ax.set_xticklabels(['No cumple', 'Cumple'], rotation=0)\n",
        "for i, v in enumerate(distribucion.values):\n",
        "    ax.text(i, v + max(distribucion.values) * 0.01, f'{v:,}', \n",
        "            ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
        "plt.tight_layout()\n",
        "ruta = os.path.join(output_dir, 'eda_01_distribucion_variable_objetivo.png')\n",
        "plt.savefig(ruta, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f'✓ 1. Distribución variable objetivo guardada en: {ruta}', flush=True)\n",
        "\n",
        "# 2. Distribución Porcentual (Pastel)\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "distribucion.plot(kind='pie', autopct='%1.1f%%', colors=['coral', 'steelblue'], ax=ax, startangle=90)\n",
        "ax.set_title('Distribución Porcentual de Variable Objetivo', fontsize=14, fontweight='bold', pad=20)\n",
        "ax.set_ylabel('')\n",
        "plt.tight_layout()\n",
        "ruta = os.path.join(output_dir, 'eda_02_distribucion_porcentual.png')\n",
        "plt.savefig(ruta, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f'✓ 2. Distribución porcentual guardada en: {ruta}', flush=True)\n",
        "\n",
        "# 3. Top 15 Columnas con Valores Faltantes\n",
        "missing = df_eda.isnull().sum()\n",
        "missing_pct = (missing / len(df_eda)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Valores_Faltantes': missing,\n",
        "    'Porcentaje': missing_pct\n",
        "})\n",
        "missing_df = missing_df[missing_df['Valores_Faltantes'] > 0].sort_values('Porcentaje', ascending=False).head(15)\n",
        "\n",
        "if len(missing_df) > 0:\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    missing_df['Porcentaje'].plot(kind='barh', color='orange', ax=ax)\n",
        "    ax.set_title('Top 15 Columnas con Valores Faltantes', fontsize=14, fontweight='bold', pad=20)\n",
        "    ax.set_xlabel('Porcentaje de Valores Faltantes', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Columna', fontsize=12, fontweight='bold')\n",
        "    ax.invert_yaxis()\n",
        "    for i, v in enumerate(missing_df['Porcentaje'].values):\n",
        "        ax.text(v + 0.5, i, f'{v:.2f}%', va='center', fontweight='bold', fontsize=9)\n",
        "    plt.tight_layout()\n",
        "    ruta = os.path.join(output_dir, 'eda_03_valores_faltantes.png')\n",
        "    plt.savefig(ruta, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f'✓ 3. Valores faltantes guardada en: {ruta}', flush=True)\n",
        "\n",
        "# 4. Boxplots de Variables Numéricas\n",
        "columnas_numericas = df_eda.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if len(columnas_numericas) > 0:\n",
        "    df_numericas = df_eda[columnas_numericas[:6]].copy()\n",
        "    fig, ax = plt.subplots(figsize=(14, 8))\n",
        "    df_numericas.boxplot(ax=ax, rot=45)\n",
        "    ax.set_title('Distribución de Variables Numéricas (Top 6)', fontsize=14, fontweight='bold', pad=20)\n",
        "    ax.set_ylabel('Valor', fontsize=12, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    ruta = os.path.join(output_dir, 'eda_04_boxplots_variables_numericas.png')\n",
        "    plt.savefig(ruta, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f'✓ 4. Boxplots variables numéricas guardada en: {ruta}', flush=True)\n",
        "\n",
        "# 5. Histogramas de Variables Numéricas\n",
        "if len(columnas_numericas) > 0:\n",
        "    df_numericas = df_eda[columnas_numericas[:6]].copy()\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    axes = axes.flatten()\n",
        "    for i, col in enumerate(df_numericas.columns):\n",
        "        if i < len(axes):\n",
        "            df_numericas[col].hist(bins=30, ax=axes[i], color='steelblue', edgecolor='black')\n",
        "            axes[i].set_title(f'{col}', fontsize=11, fontweight='bold')\n",
        "            axes[i].set_xlabel('Valor', fontsize=10)\n",
        "            axes[i].set_ylabel('Frecuencia', fontsize=10)\n",
        "            axes[i].grid(alpha=0.3)\n",
        "    plt.suptitle('Histogramas de Variables Numéricas (Top 6)', fontsize=14, fontweight='bold', y=0.995)\n",
        "    plt.tight_layout()\n",
        "    ruta = os.path.join(output_dir, 'eda_05_histogramas_variables_numericas.png')\n",
        "    plt.savefig(ruta, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f'✓ 5. Histogramas variables numéricas guardada en: {ruta}', flush=True)\n",
        "\n",
        "# 6. Distribución de TARIFA_NBO\n",
        "if columna_tarifa in df_eda.columns:\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    df_eda[columna_tarifa].notna().value_counts().plot(kind='bar', color=['red', 'green'], ax=ax)\n",
        "    ax.set_title(f'Distribución de {columna_tarifa}', fontsize=14, fontweight='bold', pad=20)\n",
        "    ax.set_xlabel('Tiene Valor', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Frecuencia', fontsize=12, fontweight='bold')\n",
        "    ax.set_xticklabels(['Faltante', 'Presente'], rotation=0)\n",
        "    for i, v in enumerate(df_eda[columna_tarifa].notna().value_counts().values):\n",
        "        ax.text(i, v + max(df_eda[columna_tarifa].notna().value_counts().values) * 0.01, f'{v:,}', \n",
        "                ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
        "    plt.tight_layout()\n",
        "    ruta = os.path.join(output_dir, 'eda_06_distribucion_tarifa_nbo.png')\n",
        "    plt.savefig(ruta, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f'✓ 6. Distribución TARIFA_NBO guardada en: {ruta}', flush=True)\n",
        "\n",
        "# 7. Distribución de Rentabilizo\n",
        "if columna_rentabilizacion in df_eda.columns:\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    df_eda[columna_rentabilizacion].notna().value_counts().plot(kind='bar', color=['red', 'green'], ax=ax)\n",
        "    ax.set_title(f'Distribución de {columna_rentabilizacion}', fontsize=14, fontweight='bold', pad=20)\n",
        "    ax.set_xlabel('Tiene Valor', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Frecuencia', fontsize=12, fontweight='bold')\n",
        "    ax.set_xticklabels(['Faltante', 'Presente'], rotation=0)\n",
        "    for i, v in enumerate(df_eda[columna_rentabilizacion].notna().value_counts().values):\n",
        "        ax.text(i, v + max(df_eda[columna_rentabilizacion].notna().value_counts().values) * 0.01, f'{v:,}', \n",
        "                ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
        "    plt.tight_layout()\n",
        "    ruta = os.path.join(output_dir, 'eda_07_distribucion_rentabilizo.png')\n",
        "    plt.savefig(ruta, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f'✓ 7. Distribución Rentabilizo guardada en: {ruta}', flush=True)\n",
        "\n",
        "# 8. Distribución de Tipos de Datos\n",
        "tipos_datos = df_eda.dtypes.value_counts()\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "tipos_datos.plot(kind='bar', color='purple', ax=ax)\n",
        "ax.set_title('Distribución de Tipos de Datos', fontsize=14, fontweight='bold', pad=20)\n",
        "ax.set_xlabel('Tipo de Dato', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Cantidad de Columnas', fontsize=12, fontweight='bold')\n",
        "ax.tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate(tipos_datos.values):\n",
        "    ax.text(i, v + max(tipos_datos.values) * 0.01, str(v), \n",
        "            ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
        "plt.tight_layout()\n",
        "ruta = os.path.join(output_dir, 'eda_08_distribucion_tipos_datos.png')\n",
        "plt.savefig(ruta, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f'✓ 8. Distribución tipos de datos guardada en: {ruta}', flush=True)\n",
        "\n",
        "# 9. Resumen Estadístico del Dataset\n",
        "resumen_stats = {\n",
        "    'Total Filas': len(df_eda),\n",
        "    'Total Columnas': len(df_eda.columns),\n",
        "    'Columnas Numéricas': len(columnas_numericas),\n",
        "    'Columnas Categóricas': len(df_eda.columns) - len(columnas_numericas),\n",
        "    'Clase 1 (Cumple)': int(distribucion.get(1, 0)),\n",
        "    'Clase 0 (No cumple)': int(distribucion.get(0, 0))\n",
        "}\n",
        "stats_df = pd.DataFrame(list(resumen_stats.items()), columns=['Métrica', 'Valor'])\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "stats_df.set_index('Métrica')['Valor'].plot(kind='barh', color='teal', ax=ax)\n",
        "ax.set_title('Resumen Estadístico del Dataset', fontsize=14, fontweight='bold', pad=20)\n",
        "ax.set_xlabel('Valor', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Métrica', fontsize=12, fontweight='bold')\n",
        "ax.invert_yaxis()\n",
        "for i, v in enumerate(stats_df['Valor'].values):\n",
        "    ax.text(v + max(stats_df['Valor'].values) * 0.01, i, f'{v:,}', \n",
        "            va='center', fontweight='bold', fontsize=11)\n",
        "plt.tight_layout()\n",
        "ruta = os.path.join(output_dir, 'eda_09_resumen_estadistico.png')\n",
        "plt.savefig(ruta, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f'✓ 9. Resumen estadístico guardada en: {ruta}', flush=True)\n",
        "\n",
        "print('\\n✓ Paso 13 completado exitosamente')\n",
        "print(f'\\nTodas las visualizaciones EDA individuales están guardadas en: {output_dir}/', flush=True)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
